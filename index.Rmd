---
title: "Simulation study of statistical methods for comparing groups"
description: |
  Examples and informal evaluations of various statistical significance tests
  for comparing observations generated from different distributions and families
author:
  - name: Mikhail Popov
    url: https://mpopov.com/
    affiliation: Wikimedia Foundation
    affiliation_url: https://wikimediafoundation.org/
date: 2020-09-21
bibliography: bibliography.bib
repository_url: https://github.com/bearloga/comparing-groups
citation_url: https://people.wikimedia.org/~bearloga/notes/comparing-groups/
slug: popov2020comparing
creative_commons: CC BY-SA
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
    df_print: kable
nocite: |
  @Rpkg-stats, @Rpkg-glue, @Rpkg-purrr, @Rpkg-furrr, @Rpkg-tidyr, @Rpkg-dplyr,
  @Rpkg-ggplot2, @Rpkg-distributional, @Rpkg-ggdist, @Rpkg-patchwork, @Rpkg-gt
---
```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(echo = FALSE, digits = 3, message = FALSE, warning = FALSE)
options(digits = 4)
library(future)
plan(multiprocess(workers = getOption("mc.cores", 2)))
```

```{css}
a.wiki-preview {
    color: #0645ad;
    text-decoration: none;
    border-bottom: 1px dotted #0645ad;
}
.wiki-preview::after {
    font-family: serif;
    content: " W";
    vertical-align: super;
    font-size: 6pt;
}
```

<!-- https://chimeces.com/context-cards/ -->
<script src="context-cards.js"></script>

```{r packages}
suppressPackageStartupMessages({
  library("glue")
  library("purrr")
  library("furrr")
  library("tidyr")
  library("dplyr")
  library("ggplot2")
  library("distributional")
  library("ggdist")
  library("patchwork")
  library("gt")
})

theme_set(theme_ggdist())
```

# Key Takeaways

- t-test is OK to perform for non-normal data and has the same false positive rate across different distributions, including skewed ones
- t-test has the greatest power of the bunch, with the small effect detected more often than when the other two tests were used with the same dataset
- with a per-group sample size of 100+, all three tests offer the same conclusion, so when working with larger samples just use a t-test because it's easier to interpret/communicate and to perform (computationally speaking)

**NOTE**: Only some of the code has been included on this page. The full code (including chunks responsible for creating tables and charts) can be found in [the R Markdown source document](https://github.com/bearloga/comparing-groups/blob/main/index.Rmd).

# Introduction

The two most difficult problems in statistics are:

- $p \gg N$ (when the number of dimensions greatly exceeds the sample size)
- the question "Can I use a <a href="https://en.wikipedia.org/wiki/Student's_t-test" class='wiki-preview' data-wiki-lang='en' data-wiki-title="Student's t-test">t-test</a> here?"

There is a wealth of literature on methods and best practices for dealing with high dimensionality, but there is not a lot of consistency when it comes to knowing when a t-test is appropriate and recommending its use in practical settings. There are two schools of thought:

- there are those who assert that the data needs to be normally-distributed and a non-parametric test should be employed otherwise
- there are those who assert that the distributional assumptions of a t-test apply to the test statistic only; so the test can be employed even with skewed data

Pananos [-@pananos2020nothing] started his blog post with:

> I hate the objection “I can’t use the t-test, my data aren’t normal”. I see it all the time on Cross Validated when a data analyst is tasked with analyzing an experiment after it has been performed. They have piles of data, thousands of observations, and they have no idea what to do with it. They know of the t-test, but they erroneously believe (through no fault of their own) that the t-test is only valid if their data are normal.

Honestly, it wasn't until I read that post that I questioned my own belief that I've been taught. It's easy to see how one might be led to believe this -- to be indoctrinated into that first school of thought.

> The *t* tests are based on an assumption that data come from the normal distribution

writes Dalgaard [-@dalgaard2008introductory].

> We shall suppose that the variables $X_1, \ldots, X_n$ form a random sample from a normal distribution

write DeGroot & Schervish [-@degroot2012probability].

> Let $X_1, \ldots, X_n$ be a random sample from a $n(\mu_X, \sigma^2_X)$...

write Casella & Berger [-@casella2002statistical].

These notes are an extension of Pananos' post on Cross Validated [-@434921]. Here I examine how the data generating process (in this case the distribution from which the observations are drawn) and the choice of statistical test (t-test and its non-parametric alternatives <a href="https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test" class='wiki-preview' data-wiki-lang='en' data-wiki-title="Mann–Whitney U test">Wilcoxon-Mann-Whitney test</a> and <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test" class='wiki-preview' data-wiki-lang='en' data-wiki-title="Kolmogorov–Smirnov test">Kolmogorov-Smirnov test</a>) affect the results. The four scenarios (distributions) are:

```{r}
tibble(
  scenario = c("a", "b", "c", "d"),
  distribution = map_chr(
    c("Normal", "Poisson", "Gamma", "Beta"),
    function(distribution) {
      return(glue("<a href=\"https://en.wikipedia.org/wiki/{distribution}_distribution\" class='wiki-preview' data-wiki-lang='en' data-wiki-title='{distribution} distribution'>{distribution}</a>"))
    }
  )
) %>%
  gt() %>%
  fmt(columns = vars("distribution"), fns = function(x) {
    map_chr(x, html)
  }) %>%
  cols_align("center") %>%
  cols_label(scenario = "Scenario", distribution = "Distribution")
```

# Methods

## Data simulation

```{r plot-distributions}
group_order <- c("control", "none", "small", "medium", "large")
plot_distributions <- function(sim_params) {
  # Normal:
  p1 <- sim_params$a %>%
    map_dfr(as_tibble, .id = "group") %>%
    mutate(group = factor(group, group_order)) %>%
    ggplot(aes(dist = dist_normal(mu = mu, sigma = sigma), y = group)) +
    stat_dist_interval() +
    ggtitle("a (Normal)")
  # Poisson:
  p2 <- sim_params$b %>%
    map_dfr(as_tibble, .id = "group") %>%
    mutate(group = factor(group, group_order)) %>%
    ggplot(aes(dist = dist_poisson(lambda = lambda), y = group)) +
    stat_dist_interval() +
    ggtitle("b (Poisson)")
  # Gamma:
  p3 <- sim_params$c %>%
    map_dfr(as_tibble, .id = "group") %>%
    mutate(group = factor(group, group_order)) %>%
    ggplot(aes(dist = dist_gamma(shape = shape, rate = rate), y = group)) +
    stat_dist_interval() +
    ggtitle("c (Gamma)")
  # Beta:
  p4 <- sim_params$d %>%
    map_dfr(as_tibble, .id = "group") %>%
    mutate(group = factor(group, group_order)) %>%
    ggplot(aes(dist = dist_beta(shape1 = shape1, shape2 = shape2), y = group)) +
    scale_x_continuous(limits = c(0, 1)) +
    stat_dist_interval() +
    ggtitle("d (Beta)")
  return(wrap_plots(p1, p2, p3, p4, nrow = 2, ncol = 2))
}
```

The idea here is to specify different centers and shapes of the various distributions for every group, except the "none" group which we include for measuring <a href="https://en.wikipedia.org/wiki/Type_I_and_type_II_errors" class='wiki-preview' data-wiki-lang='en' data-wiki-title="Type I and type II errors">type I</a> error (aka false positive rate (FPR), aka false rejection rate (FRR), aka $\alpha$): the probability of incorrectly rejecting the null hypothesis when the null hypothesis is true. The null hypothesis will be true for the "none" group because it has the exact same distribution as the "control" group.

```{r simulation-parameters, cache=TRUE, echo=TRUE}
simulation_parameters <- list(
  # Normal:
  a = list(
    control = list(mu = 20, sigma = 2),
    none = list(mu = 20, sigma = 2),
    small = list(mu = 22, sigma = 2),
    medium = list(mu = 26, sigma = 2),
    large = list(mu = 28, sigma = 2)
  ),
  # Poisson:
  b = list(
    control = list(lambda = 2),
    none = list(lambda = 2),
    small = list(lambda = 4),
    medium = list(lambda = 6),
    large = list(lambda = 8)
  ),
  # Gamma:
  c = list(
    control = list(shape = 2, rate = 1 / 100),
    none = list(shape = 2, rate = 1 / 100),
    small = list(shape = 4, rate = 1 / 100),
    medium = list(shape = 6, rate = 1 / 100),
    large = list(shape = 8, rate = 1 / 100)
  ),
  # Beta:
  d = list(
    control = list(shape1 = 10, shape2 = 20),
    none = list(shape1 = 10, shape2 = 20),
    small = list(shape1 = 15, shape2 = 20),
    medium = list(shape1 = 20, shape2 = 20),
    large = list(shape1 = 25, shape2 = 20)
  )
)
```

Let's visualize what the different groups' distributions look like (using the [interval plot](https://mjskay.github.io/ggdist/reference/stat_dist_slabinterval.html)) with those parameters:

<aside>
These plots were created with [`{ggplot2}`](https://ggplot2.tidyverse.org/), [`{ggdist}`](https://mjskay.github.io/ggdist/), [`{distributional}`](https://pkg.mitchelloharawild.com/distributional/), and [`{patchwork}`](https://patchwork.data-imaginist.com/) packages.
</aside>

```{r, layout="l-body-outset", fig.width=7, fig.height=5, dependson=c("simulation-parameters")}
plot_distributions(simulation_parameters)
```

Next, we define a `simulate_data()` function for simulating data given the various parameters.

```{r simulate-data, echo=TRUE}
simulate_data <- function(N, sim_params) {
  # sim_params is a list of scenarios; each scenario will be a named list with
  # "control", "none", etc.
  simulations <- list(
    a = map(sim_params[["a"]], ~ rnorm(N, mean = .x$mu, sd = .x$sigma)),
    b = map(sim_params[["b"]], ~ rpois(N, lambda = .x$lambda)),
    c = map(sim_params[["c"]], ~ rgamma(N, shape = .x$shape, rate = .x$rate)),
    d = map(sim_params[["d"]], ~ rbeta(N, shape1 = .x$shape1, shape2 = .x$shape2))
  )
  # This will create a tibble with columns "scenario", "group", "y":
  tidy_simulations <- simulations %>%
    # Turn each scenario into a (wide) tibble with each group as a column:
    map(as_tibble) %>%
    # Then we want to reshape it into a long tibble:
    map_dfr(
      pivot_longer,
      cols = all_of(names(sim_params[[1]])),
      # ^ in case only "control" & "none" are provided
      names_to = "group",
      values_to = "y",
      .id = "scenario"
    )
  return(tidy_simulations)
}
```

```{r example-data, cache=TRUE, echo=TRUE, dependson=c("simulation-parameters")}
set.seed(20200921)
n_example_size <- 2000
example_data <- simulate_data(N = n_example_size, sim_params = simulation_parameters)
```

Here are the distributions of the data in the different scenarios from this one single simulated dataset of `r n_example_size` randomly generated observations **per group**:

```{r, layout="l-body-outset", fig.width=7, fig.height=5}
example_data %>%
  mutate(group = factor(group, group_order)) %>%
  split(., .$scenario) %>%
  imap(function(sim_data, scenario) {
    ggplot(sim_data, aes(x = y, y = group)) +
      stat_halfeye(adjust = 2) +
      ggtitle(scenario)
  }) %>%
  wrap_plots(nrow = 2, ncol = 2)
```

## Data analysis

Given a single (simulated) dataset, we need to perform each of the three statistical tests on each pair of groups, always comparing to the control group. This work is captured by the `analyze_data()` function:

<aside>
The results of the tests are then tidied-up via the [`{broom}`](https://broom.tidymodels.org/) package.
</aside>

```{r analyze-data, echo=TRUE}
analyze_data <- function(simulated_data) {
  comparison_groups <- c("none", "small", "medium", "large")
  names(comparison_groups) <- paste("control vs", comparison_groups)
  scenarios <- set_names(c("a", "b", "c", "d"))
  
  comparisons <- map_dfr(
    scenarios,
    function(scenario) {
      map_dfr(
        comparison_groups, function(comparison_group) {
          sim_data <- simulated_data[simulated_data[["scenario"]] == scenario, ] %>%
            filter(group %in% c("control", comparison_group)) %>%
            mutate(group = factor(group))
          tests <- list(
            t_test = t.test(y ~ group, data = sim_data),
            wilcox_test = wilcox.test(y ~ group, data = sim_data),
            ks_test = ks.test(
              x = sim_data$y[sim_data$group == "control"],
              y = sim_data$y[sim_data$group != "control"]
            )
          )
          results <- map_dfr(tests, broom::tidy, .id = "statistical_test")
          return(select(results, statistical_test, statistic, p.value, method))
        },
        .id = "comparison"
      )
    },
    .id = "scenario"
  )
  
  return(comparisons)
}
```

## Putting it all together

Now we want to simulate and analyze thousands of times and also across different sample sizes (per group), to study the impact of sample sizes on robustness of those tests:

```{r analysis-results, dependson=c("simulation-parameters"), echo=FALSE, cache=TRUE}
# Number of replications to perform:
n_replications <- 100000
# Sample sizes to explore at each replication:
sample_sizes <- c(25, 50, 100)

if (fs::file_exists("analysis_results.rds")) {
  # Retrieve cached results:
  analysis_results <- readr::read_rds("analysis_results.rds")
} else {
  analysis_results <- seq(1, n_replications) %>%
    future_map_dfr(
      function(replication) {
        # At each rep, simulate & analyze data w/ various sample sizes:
        analysis_result <- sample_sizes %>%
          set_names() %>%
          map(simulate_data, sim_params = simulation_parameters) %>%
          map_dfr(analyze_data, .id = "sample_size")
        return(analysis_result)
      },
      .id = "replication",
      .progress = TRUE,
      .options = future_options(
        packages = c("tidyr", "dplyr", "broom"),
        seed = 42L
      )
    )
  # Cache results:
  readr::write_rds(analysis_results, "analysis_results.rds", compress = "gz")
}
```
``` r
set.seed(42L)
# Number of replications to perform:
n_replications <- 100000
# Sample sizes to explore at each replication:
sample_sizes <- c(25, 50, 100)
analysis_results <- seq(1, n_replications) %>%
  map_dfr(
    function(replication) {
      # At each rep, simulate & analyze data w/ various sample sizes:
      analysis_result <- sample_sizes %>%
        set_names() %>%
        map(simulate_data, sim_params = simulation_parameters) %>%
        map_dfr(analyze_data, .id = "sample_size")
      return(analysis_result)
    },
    .id = "replication"
  )
```
```{r analysis-results-2, cache=TRUE, dependson=c("analysis-results")}
analysis_results <- analysis_results %>%
  mutate(
    sample_size = as.integer(sample_size),
    scenario = factor(
      scenario,
      c("a", "b", "c", "d"),
      c("a (Normal)", "b (Poisson)", "c (Gamma)", "d (Beta)")
    ),
    statistical_test_f = factor(
      statistical_test,
      c("t_test", "wilcox_test", "ks_test"),
      c("Student's t-test", "Wilcoxon-Mann-Whitney test", "Kolmogorov-Smirnov Test")
    )
  )
```

# Results

For each combination of scenario and sample size, we calculate the proportion of replications where the null hypothesis would be rejected (based on the p-value and significance level $\alpha = 0.05$). As a reminder, in case of the "control vs none" comparison, the null hypothesis (that the means/distributions are the same between the two groups) *is* true, so we would expect the proportion to equal $\alpha$ -- the probability of a false rejection.

<aside>
The following table summarizing those results is created with the [`{gt}`](https://gt.rstudio.com/) package.
</aside>

```{r rejection-rates, cache=TRUE, dependson=c('analysis-results-2')}
rejection_rates <- analysis_results %>%
  group_by(scenario, statistical_test_f, sample_size, comparison) %>%
  summarize(reject = mean(p.value <= 0.05), .groups = "drop") %>%
  pivot_wider(names_from = "comparison", values_from = "reject")
```
```{r, layout="l-body-outset"}
rejection_rates %>%
  mutate(sample_size = paste0("N = ", sample_size)) %>%
  group_by(scenario, sample_size) %>%
  gt(row_group.sep = ", ") %>%
  tab_header(
    title = "Two-sided null hypothesis rejection rate",
    subtitle = html("At &alpha;=0.05 significance level")
  ) %>%
  fmt_percent(columns = starts_with("control vs"), decimals = 1) %>%
  cols_move(
    columns = vars("control vs none", "control vs small", "control vs medium", "control vs large"),
    after = vars("statistical_test_f")
  ) %>%
  cols_label(statistical_test_f = "Statistical test by scenario") %>%
  cols_align(align = "left", columns = vars("statistical_test_f")) %>%
  tab_style(
    style = list(cell_text(weight = "bold")),
    locations = cells_row_groups()
  ) %>%
  tab_style(
    style = list(cell_text(indent = px(20))),
    locations = cells_body(columns = vars("statistical_test_f"))
  )
```

```{r agreement-rates, cache=TRUE, dependson=c('analysis-results-2')}
agreement_rates <- analysis_results %>%
  mutate(reject = p.value <= 0.05) %>%
  select(scenario, comparison, sample_size, replication, statistical_test, reject) %>%
  pivot_wider(names_from = "statistical_test", values_from = "reject") %>%
  mutate(
    all_agreed = t_test & wilcox_test & ks_test,
    tw_agreed = t_test & wilcox_test,
    tks_agreed = t_test & ks_test,
    wks_agreed = wilcox_test & ks_test
  ) %>%
  pivot_longer(
    cols = ends_with("agreed"), names_pattern = "^([a-z]+)_agreed$",
    names_to = "tests", values_to = "agreed"
  ) %>%
  group_by(scenario, sample_size, comparison, tests) %>%
  summarize(agreement_rate = mean(agreed), .groups = "drop")
```
```{r, layout="l-body-outset"}
agreement_rates %>%
  mutate(
    sample_size = paste0("N = ", sample_size),
    tests = factor(tests, c("all", "tw", "tks", "wks"), c("All three", "t-test and Wilcox", "t-test and K-S", "Wilcox and K-S"))
  ) %>%
  pivot_wider(names_from = "comparison", values_from = "agreement_rate") %>%
  group_by(scenario, sample_size) %>%
  gt(row_group.sep = ", ") %>%
  tab_header(
    title = "Rate of agreement on rejecting or failing to reject",
    subtitle = html("At &alpha;=0.05 significance level")
  ) %>%
  fmt_percent(columns = starts_with("control vs"), decimals = 1) %>%
  cols_move(
    columns = vars("control vs none", "control vs small", "control vs medium", "control vs large"),
    after = vars("tests")
  ) %>%
  tab_style(
    style = list(cell_text(weight = "bold")),
    locations = cells_row_groups()
  ) %>%
  cols_align(align = "left", columns = vars("tests")) %>%
  cols_label(tests = "Statistical tests") %>%
  tab_style(
    style = list(cell_text(indent = px(20))),
    locations = cells_body(columns = vars("tests"))
  )
```

```{r, layout="l-body-outset", fig.width=6, fig.height=3, eval=FALSE}
analysis_results %>%
  mutate(
    sample_size_f = factor(sample_size, sample_sizes, paste("N =", sample_sizes)),
    comparison_f = factor(comparison, paste("control vs", c("none", "small", "medium", "large")))
  ) %>%
  filter(statistical_test == "t_test") %>%
  ggplot(aes(x = statistic, y = comparison_f)) +
  stat_halfeye() +
  facet_wrap( ~ sample_size_f, nrow = 1, scales = "free_x") +
  labs(title = "Distributions of t-test statistics", y = NULL, x = "Test statistic") +
  theme(legend.position = "bottom")
```

# Discussion

Across all the non-normal scenarios (b, c, d) it looks like the t-test is okay to perform for non-normal data. Across all the scenarios, it has the same false positive rate (type I error) across different distributions, including skewed ones. The conclusions from the t-test and its most similar non-parametric alternative -- the Wilcoxon-Mann-Whitney test -- are in greatest agreement, relative to comparing conclusions from Kolmogorov-Smirnov Test. Almost consistently, t-test has the greatest power of the three across different sample sizes, with the small effect detected more often than when the other two tests were used with the same dataset. With a per-group sample size of $N=100$ or greater, all three tests offer the same conclusion -- so when working with larger samples just use a t-test because it's easier to interpret or communicate and to execute.

# Acknowledgements {.appendix}

You might have noticed a few blue links with "W"s on this page. Those are links to the Wikipedia articles on those topics and if you hover over them, you will see a preview of the article. This is possible with the [ContextCards library](https://chimeces.com/context-cards/), based on the [Popups extension for MediaWiki](https://www.mediawiki.org/wiki/Extension:Popups).
